# Feature Specification: CloudyIntel Architecture Website

**Feature Branch**: `001-build-cloudyintel-website`  
**Created**: 2026-02-28  
**Status**: Draft  
**Input**: User description: "Build a website for CloudyIntel that automates cloud architecture generation and validation with provider-specific AWS/Azure loops, recursive evaluator-optimizer refinement, RAG-based validation, and transparent graph-based workflow visibility."

## Clarifications

### Session 2026-02-28

- Q: What stopping policy should govern iterative refinement loops? → A: Max 3 refinement iterations per provider, then stop and return best result with unresolved findings clearly marked.
- Q: What authentication model should this feature use? → A: No authentication (local/dev use only).
- Q: How should run history be persisted? → A: Persist run history locally per project/workspace (survives refresh/restart).
- Q: What should happen when validation evidence retrieval is unavailable? → A: Block acceptance for affected provider, return best draft as not validated, and show required remediation actions.

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Generate Validated Architecture by Provider (Priority: P1)

As a Cloud Solution Architect, I submit a workload query and receive an architecture response that is generated by domain agents, validated with evidence, and iteratively refined until accepted, with clear separation for AWS and Azure.

**Why this priority**: This is the primary business value of CloudyIntel; without a validated provider-specific solution, the product does not fulfill its core purpose.

**Independent Test**: Can be fully tested by submitting one AWS-only query and one Azure-only query, verifying each returns an accepted architecture after generation and validation rounds with no provider cross-contamination.

**Acceptance Scenarios**:

1. **Given** the user selects AWS scope and submits a query, **When** the system runs domain generation and validation loops, **Then** the final response contains AWS-specific architecture decisions only and includes validation outcomes.
2. **Given** the user selects Azure scope and submits a query, **When** initial validation fails, **Then** the system performs iterative refinement up to 3 rounds and returns the best result with unresolved findings clearly marked if acceptance is not reached.
3. **Given** the user selects both providers, **When** processing completes, **Then** the system returns two clearly separated accepted solutions (AWS and Azure), each with its own validation trail.

---

### User Story 2 - Observe Agent Workflow and Hand-offs (Priority: P2)

As a Cloud Solution Architect, I can see a live graph of orchestration and validation states, including active agent nodes, hand-offs, and whether the system is currently optimizing due to validation feedback.

**Why this priority**: Transparency increases trust and usability; architects must understand how and why recommendations are produced.

**Independent Test**: Can be tested by running a query that triggers at least one failed validation and confirming the UI shows node transitions, hand-offs, and optimize/retry state changes in sequence.

**Acceptance Scenarios**:

1. **Given** a query is in progress, **When** a domain generator starts work, **Then** the corresponding graph node state changes to active and is visible to the user.
2. **Given** validation feedback is produced, **When** the system routes it back to generation, **Then** the graph view clearly indicates an optimization cycle instead of completion.

---

### User Story 3 - Review Validation Evidence and Feedback (Priority: P3)

As a Cloud Solution Architect, I can inspect domain-level validation evidence and actionable feedback so I can understand rejected decisions and adjust constraints if needed.

**Why this priority**: Evidence visibility supports auditability and practical decision-making, but is secondary to receiving a validated architecture.

**Independent Test**: Can be tested by forcing a known validation failure and confirming each rejected domain decision includes reason, evidence reference, and improvement guidance.

**Acceptance Scenarios**:

1. **Given** a domain recommendation fails validation, **When** feedback is generated, **Then** the user can view domain-specific failure reasons and concrete improvement notes.
2. **Given** a recommendation passes validation, **When** the user opens evidence details, **Then** the user sees why the recommendation was accepted and which source set supported it.

---

### Edge Cases

- User requests both AWS and Azure, but one provider reaches stopping rules while the other succeeds.
- Retrieved validation evidence is unavailable or insufficient for one or more domains; affected provider output must remain not validated.
- Domain generator outputs conflict at synthesis time (e.g., incompatible network and database assumptions).
- Validation repeatedly fails beyond the 3-iteration per-provider limit.
- User switches provider scope or query constraints during an active run.
- One domain agent times out while others complete.

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST accept architecture queries from a user and classify requested provider scope as AWS, Azure, or both.
- **FR-002**: System MUST decompose each query into compute, storage, network, and database domain tasks.
- **FR-003**: System MUST execute domain generation independently for each requested provider.
- **FR-004**: System MUST combine domain generation outputs into one coherent architecture proposal per provider.
- **FR-005**: System MUST run domain validation for each architecture proposal using evidence grounded in official provider documentation sources.
- **FR-006**: System MUST aggregate domain validation results into unified provider-level feedback.
- **FR-007**: System MUST re-run generation with feedback when validation fails, with a maximum of 3 refinement iterations per provider run.
- **FR-008**: System MUST keep AWS and Azure generation/validation loops fully separated in state, evidence, and output.
- **FR-009**: System MUST present provider-specific final responses as separate architecture outputs when both providers are requested.
- **FR-010**: System MUST display a graph-based workflow view showing node state, active step, and agent hand-offs during execution.
- **FR-011**: System MUST visibly distinguish states for generating, validating, optimizing, accepted, and stopped.
- **FR-012**: System MUST expose domain-level validation evidence summaries for accepted and rejected decisions.
- **FR-013**: System MUST expose actionable remediation notes for each rejected validation finding.
- **FR-014**: System MUST preserve a run history that links each refinement round to the feedback that triggered it.
- **FR-015**: System MUST surface partial results and failure reasons when any domain cannot complete.
- **FR-018**: System MUST return the best available provider result after the 3-iteration limit and clearly mark unresolved findings.
- **FR-019**: System MUST operate without user authentication for this feature and be explicitly limited to local/development use.
- **FR-020**: System MUST display a visible banner indicating that authentication is disabled and the environment is local/dev only.
- **FR-021**: System MUST persist run history locally per project/workspace so history survives browser refresh and local service restart.
- **FR-022**: System MUST keep persisted run history provider-separated (AWS and Azure) and query-traceable by run identifier.
- **FR-023**: System MUST NOT mark a provider run as accepted when required validation evidence is unavailable for any required validator domain.
- **FR-024**: When evidence is unavailable, system MUST return the best draft for the affected provider as not validated and include required remediation actions.
- **FR-016**: System MUST provide user-facing documentation for query usage, provider scope behavior, evidence interpretation, and troubleshooting.
- **FR-017**: System MUST provide operator-facing documentation for run monitoring, failure handling, and content-source refresh procedures.

### Key Entities *(include if feature involves data)*

- **Architecture Query**: User-submitted request containing goals, constraints, provider scope, and optional optimization preferences.
- **Provider Run**: Full execution context for one provider (AWS or Azure), including status, iteration count, timestamps, and terminal outcome.
- **Domain Task**: Unit of work for one architecture domain (compute/storage/network/database) tied to a provider run.
- **Domain Proposal**: Generated recommendation for a domain, including rationale and dependencies.
- **Architecture Proposal**: Provider-level combined design assembled from domain proposals.
- **Validation Finding**: Domain or cross-domain validation result with outcome (pass/fail/warn), reason, and severity.
- **Evidence Reference**: Pointer to supporting source content used in validation reasoning.
- **Refinement Feedback**: Actionable correction guidance produced by validation and fed back into generation.
- **Workflow Node State**: UI-visible state record for each orchestration step and hand-off.
- **Run Event**: Timestamped event for state transitions, retries, failures, and completion.

## Assumptions

- A maintained corpus of official AWS and Azure documentation evidence is available locally for retrieval.
- The product supports two user-facing provider scopes: AWS and Azure; other cloud providers are out of scope for this feature.
- Users can submit one query at a time per active session.
- Stopping policy is fixed at 3 refinement iterations per provider run; if validation still fails, return best available result with unresolved findings flagged.
- Authentication is intentionally disabled for this feature phase and deployment target is local/development only.
- Run history is stored locally in a workspace-scoped store; no shared multi-user persistence is included in this phase.
- Architecture outputs are advisory recommendations and require human review before deployment.

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: 90% of valid user queries produce at least one provider-level architecture response within 5 minutes.
- **SC-002**: 95% of completed runs show at least one full generation-to-validation cycle in the visible workflow timeline.
- **SC-003**: For dual-provider queries, 100% of outputs are clearly separated by provider with no mixed provider recommendations.
- **SC-004**: 90% of rejected validation findings include remediation guidance that users rate as actionable.
- **SC-005**: 95% of workflow node transitions appear in the UI within 2 seconds of backend state change.
- **SC-006**: 85% of pilot users can accurately explain why a final recommendation was accepted based on visible evidence summaries.
